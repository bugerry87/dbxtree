# DBXTree

A semi-lossless tree-based geometry compression model through deep range coding using features from populated child-nodes.

DBXTree decomposes a point cloud to a KD-Tree while the dimension *K* is dynamically adjusted to the point cloud's proportion.
The breadth-first order of the KD-Tree generates a code of 8bit, 4bit and 2bit symbols.
Our deep learning model estimates the probability of each symbol based on extracted features from populated child-node candidates.
These child-nodes are generated by assuming fully occupation of the next tree-layer.
Based on that, the deep learning model learns the likelyhood between the features and the probability of existens per node.
These probabilities are fed to a range coder and compresses the final bit-stream.

## Requirements

- GPU with CUDA and huge memory
- Python v3.8
- Tensorflow v2.3
- Tensorflow-Probability v0.11.0
- Tensorflow-Compression v2.0b1

## Dataset

This project was namely designed for KITTI-like datasets.
It expects point clouds in a headless byte-stream of serialized 32bit floats, where each point has 4\*32bit floats of *(x,y,z,i)*, where *i* is intensity.
However, intensity *i* is not used by this model.
The input-shape can be adjusted via command arguments.

Therefore download the KITTI dataset, e.g.:

- bbb

Then, define an index file for each stage such as `train_index.txt`, `val_index.txt` and `test_index.txt`.
These index files are a simple line-wise list of pathes to the actual point cloud file, for instance:

```
./data/kitti/00/velodyne/000000.bin
./data/kitti/00/velodyne/000001.bin
./data/kitti/00/velodyne/000002.bin
./data/kitti/00/velodyne/000003.bin
./data/kitti/00/velodyne/000004.bin
./data/kitti/00/velodyne/000005.bin
./data/kitti/00/velodyne/000006.bin
./data/kitti/00/velodyne/000007.bin
./data/kitti/00/velodyne/000008.bin
./data/kitti/00/velodyne/000009.bin
...
```

However, it is recommended to shuffle the data.

## Training

For training we recommend a Linux system and a GPU with huge memory.
Since a KITTI point cloud can have up to 120k points per file the GPU must handle a memory peaks of round about to *(120k\*kernel_size\*layers\*4 + parameters\*4)*.

The minimal command to start this model would be:

```sh
python3 ./train_run_dbx.py -X ./data/train_index.txt -Y ./data/val_index.txt -T ./data/test_index.txt
```

This will start a training session with validation and test sequence for one epoch by default configuration.
See help output for more information and configuration:

```
python3 ./train_run_dbx.py --help
```

## Evaluation




